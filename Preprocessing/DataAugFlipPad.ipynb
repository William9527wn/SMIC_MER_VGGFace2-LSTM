{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing version 4.0\n",
    "#1. Random steps between frames within a slide window;\n",
    "#2. Variable sele_frame(slide window size)\n",
    "#3. Fixed-size of slide window(if image length smaller than the seleframe,adding frames);\n",
    "#4. Mirror flipping\n",
    "#5. Raw data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Monday May 18 10:51 2020\n",
    "Used to pre-process dataset \n",
    "@author: Keira - github.com/Keira. Bai\n",
    "a.function to divide data set into 5 groups for cross-validation, return a 5*Ndimention matrix \n",
    "b.function to extract trainiing data into fixed size sequences, sequences consist by consecuous internal\n",
    "c.function to extract validation data, sequences consist by raw image and adding frames to the same size\n",
    "d.function to read image\n",
    "\"\"\"\n",
    "import PIL\n",
    "import torch\n",
    "import glob as gb\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allocate dataset into k groups for training and validation\n",
    "def CrossAllocation(datapaths,k):\n",
    "    neg_dir = \"*/negative/*\"\n",
    "    pos_dir = \"*/positive/*\"\n",
    "    sur_dir = \"*/surprise/*\"\n",
    "    non_dir = \"non_micro/*\"\n",
    "    imgpath = \"/*.bmp\"\n",
    "    sequ_list = []\n",
    "    label_list = []\n",
    "    Val_tra_tes_list = []\n",
    "    label = 0\n",
    "    #Load in all folders adding expression\n",
    "    for vid_dir in [neg_dir, pos_dir, sur_dir, non_dir]:        \n",
    "        sequ = gb.glob(datapaths+vid_dir)#read all folders under different expression\n",
    "        sequ_list += sequ \n",
    "        label_list += [label for i in range(len(sequ))] #read in the same number of expression label\n",
    "        label += 1 #change the expression category\n",
    "    #Devide All Data into k groups\n",
    "    for i in range(k):\n",
    "        if k-i>1:\n",
    "            train_list, valid_list, train_label, valid_label = train_test_split(sequ_list, label_list, \\\n",
    "                                          test_size=1/(k-i), random_state=42)\n",
    "            Val_tra_tes_list.append([valid_list, valid_label])\n",
    "            \n",
    "        else:\n",
    "            Val_tra_tes_list.append([train_list, train_label])\n",
    "\n",
    "        sequ_list = train_list\n",
    "        label_list = train_label   \n",
    "\n",
    "    return Val_tra_tes_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data augmentation on sequence level by fixed size\n",
    "def InputImagewithSlide(tra_tes_list, seleframe = 11):    \n",
    "    imgpath = \"/*.bmp\"\n",
    "    sequ_list = []\n",
    "    \n",
    "    exp_list = []\n",
    "    step_list = [1,2,3,4,5,6]\n",
    "    expCount = [0,0,0,0,0,0,0,0]\n",
    "\n",
    "    for group in tra_tes_list:    \n",
    "        for folder, exp in zip(group[0], group[1]):\n",
    "            img_list = sorted(gb.glob(folder + imgpath))#get frames from the same one folder\n",
    "            img_len = len(img_list)\n",
    "            expCount[exp] += img_len#recording the same number of expression \n",
    "            padSeq = []\n",
    "            #repackage training sequence            \n",
    "            if img_len < seleframe: #for frames less than seleframe\n",
    "                padSeq.extend(img_list)\n",
    "                padSeq.extend(img_list[:seleframe-img_len])\n",
    "                sequ_list.append(padSeq)#append the sequence directly\n",
    "                exp_list.append(exp)\n",
    "                expCount[exp+4] += seleframe\n",
    "            else: #for frames more than seleframe\n",
    "                #control step to keep the amout balance between expressions\n",
    "                if exp == 3:#non-micro-expression\n",
    "                    step = 3#decrease the growth rate\n",
    "                else:\n",
    "                    step = 1\n",
    "                for i in range(0, img_len-seleframe+step, step):\n",
    "                        if ((img_len-i) >= (seleframe+step-1)):#sequence is enough for a seleframe+step\n",
    "                            for s in step_list:#assign inner step for frame\n",
    "                                if img_len-i >= seleframe*s: #if frame number is enough for the inner step\n",
    "                                    sequence = []\n",
    "                                    for j in range(0,seleframe*s,s):\n",
    "                                        sequence.append(img_list[j+i]) #frames for a window size  \n",
    "                                    sequ_list.append(sequence)#adding sequence for sequence list\n",
    "                                    exp_list.append(exp)\n",
    "                                    expCount[exp+4] += seleframe\n",
    "                        else:\n",
    "                            if step != 1:\n",
    "                                m = img_len-seleframe\n",
    "                                sequence = []\n",
    "                                for j in range(seleframe):\n",
    "                                    sequence.append(img_list[j+m])#adding the frames in the end part of video \n",
    "                                sequ_list.append(sequence)\n",
    "                                exp_list.append(exp)\n",
    "                                expCount[exp+4] += seleframe\n",
    "\n",
    "    return sequ_list, exp_list, expCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting out frames for valifation\n",
    "def InputImageforVal(val,seleframe):\n",
    "    imgpath = \"/*.bmp\"\n",
    "    sequ_list = []\n",
    "    exp_list = []\n",
    "    expCount = [0,0,0,0]  \n",
    "    starPoint = 0\n",
    "    endPoint = 0\n",
    "\n",
    "    for folder, exp in zip(val[0],val[1]): #val[0] for images, val[1] for expressions\n",
    "        img_list = sorted(gb.glob(folder + imgpath))#reading all images in one folder\n",
    "        img_len = len(img_list)\n",
    "        expCount[exp] += img_len #recording the number of each expression\n",
    "        #repackage images in each folder in fixed-length sequences\n",
    "        for k in range(0,img_len, seleframe):#seleframe is also the increase step\n",
    "            sequence = img_list[k:k+seleframe]\n",
    "            if len(sequence) < seleframe:                \n",
    "                sequence = img_list[img_len-seleframe:img_len]\n",
    "            sequ_list.append(sequence)\n",
    "            exp_list.append(exp)\n",
    "    \n",
    "    return sequ_list, exp_list, expCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloader the dataset after flipping \n",
    "def Reloader(train_set):\n",
    "    train_dataset = []\n",
    "    for Tset in train_set:\n",
    "        Ro_len = len(Tset[0])\n",
    "        for i in range(Ro_len):\n",
    "            train_dataset.append([Tset[0][i],Tset[1][i]])\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw data for validation\n",
    "def RawforVal(val):\n",
    "    imgpath = \"/*.bmp\"\n",
    "    sequ_list = []\n",
    "    exp_list = []\n",
    "    expCount = [0,0,0,0]      \n",
    "    \n",
    "    for folder, exp in zip(val[0],val[1]):      \n",
    "        img_list = sorted(gb.glob(folder + imgpath))\n",
    "        img_len = len(img_list)\n",
    "        expCount[exp] += img_len\n",
    "        sequ_list.append(img_list)\n",
    "        exp_list.append(exp)\n",
    "    return sequ_list, exp_list, expCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing image data to matrix\n",
    "class MicroExpDataset(data.Dataset):\n",
    "    \"\"\"Micro Expressions dataset.\"\"\" \n",
    "\n",
    "    def __init__(self, folders, labels, tran_t = None, tran_r = None):\n",
    "        \n",
    "        self.tran_t = tran_t\n",
    "        self.folders = folders\n",
    "        self.labels = labels\n",
    "        sequ_list = []\n",
    "        exp_list = []\n",
    "        exp = 0        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.folders)         \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.folders[idx]\n",
    "        exp = torch.LongTensor([self.labels[idx]])\n",
    "        sample = []\n",
    "        sampleo = []\n",
    "        samplef = []\n",
    "        for i in seq:\n",
    "            img = mpimg.imread(i)#read image\n",
    "            img = Image.fromarray(img)#transfer array image to matrix\n",
    "            #get mirror flip\n",
    "            imgf = transforms.functional.hflip(img)\n",
    "            imgf = self.tran_t(imgf)\n",
    "            samplef.append(imgf)\n",
    "            #original image\n",
    "            imgo = self.tran_t(img)             \n",
    "            sampleo.append(imgo)            \n",
    "\n",
    "        sampleo = torch.stack(sampleo, dim=0) \n",
    "        samplef = torch.stack(samplef, dim=0) \n",
    "\n",
    "        return sampleo, samplef, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapaths = \"../SMIC/SMIC_all_cropped/HS/*/\" \n",
    "# k=5\n",
    "# v=0\n",
    "# seleframe = 22\n",
    "# Val_tra_tes_list = CrossAllocation(datapaths,k)\n",
    "# batch_size = 40\n",
    "# # Detect devices\n",
    "# use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # use CPU or GPU\n",
    "\n",
    "# params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# #Set transformation\n",
    "# tran_t = transforms.Compose(\n",
    "#     [transforms.Resize([224,224]), \n",
    "#      transforms.ToTensor(),     \n",
    "#     ])\n",
    "# val = Val_tra_tes_list[v] \n",
    "# valid_list, valid_label, val_expCount = InputImageforVal(val, seleframe)\n",
    "# # for v in valid_list:\n",
    "# #     print(len(v))\n",
    "# tra_tes_list = np.delete(Val_tra_tes_list,v,axis = 0) \n",
    "# train_test_list, train_test_label, expCount = InputImagewithSlide(tra_tes_list, seleframe)#get training and testing data\n",
    "# # train_list, test_list, train_label, test_label = train_test_split(train_test_list, train_test_label, \\\n",
    "# #                                       test_size=1/k, random_state=42)\n",
    "# # train_set, test_set, valid_set = MicroExpDataset(train_list[:3], train_label[:3], tran_t=tran_t), \\\n",
    "# #                                 MicroExpDataset(test_list, test_label, tran_t=tran_t), \\\n",
    "# #                                 MicroExpDataset(valid_list, valid_label, tran_t=tran_t)\n",
    "# # train_loader = data.DataLoader(train_set, **params)\n",
    "# # # test_loader = data.DataLoader(test_set, **params)\n",
    "# # # valid_loader = data.DataLoader(valid_set, **params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
